{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Language Modeling with RNNs\n",
    "* <b>Learning Objective:</b> In this problem, you are going to implement simple recurrent neural networks to deeply understand how RNNs works.\n",
    "* <b>Provided Code:</b> We provide the skeletons of classes you need to complete. Forward checking and gradient checkings are provided for verifying your implementation as well.\n",
    "* <b>TODOs:</b> you will firstly implement a vanilla RNN to warm up, and then implement an LSTM to train a model that can generate text using your own text source (novel, lyrics etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lib.rnn.rnn import *\n",
    "from lib.rnn.layer_utils import *\n",
    "from lib.rnn.train import *\n",
    "from lib.grad_check import *\n",
    "from lib.optim import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "We will use recurrent neural network (RNN) language models for text generation.\n",
    "\n",
    "Please complete the TODOs in the function `VanillaRNN` of the file `lib/rnn/layer_utils.py` which should contain implementations of different layer types that are needed for recurrent neural networks.\n",
    "\n",
    "And then, complete the TODOs in the file `lib/rnn/rnn.py` which uses these layers to implement a text generation model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RNN: step forward (4 Pts)\n",
    "Open the file `lib/rnn/layer_utils.py`. Implement the forward and backward passes for different types of layers that are commonly used in recurrent neural networks.\n",
    "\n",
    "First complete the implementation of the function `step_forward` which implements the forward pass for a single timestep of a vanilla recurrent neural network.\n",
    "\n",
    "After doing so run the following code. You should see errors less than 1e-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "next_h error:  6.292421426471037e-09\n"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "N, D, H = 3, 10, 4\n",
    "\n",
    "rnn = VanillaRNN(D, H, init_scale=0.02, name=\"rnn_test\")\n",
    "x = np.linspace(-0.4, 0.7, num=N*D).reshape(N, D)\n",
    "prev_h = np.linspace(-0.2, 0.5, num=N*H).reshape(N, H)\n",
    "\n",
    "rnn.params[rnn.wx_name] = np.linspace(-0.1, 0.9, num=D*H).reshape(D, H)\n",
    "rnn.params[rnn.wh_name] = np.linspace(-0.3, 0.7, num=H*H).reshape(H, H)\n",
    "rnn.params[rnn.b_name] = np.linspace(-0.2, 0.4, num=H)\n",
    "\n",
    "next_h, _ = rnn.step_forward(x, prev_h)\n",
    "expected_next_h = np.asarray([\n",
    "  [-0.58172089, -0.50182032, -0.41232771, -0.31410098],\n",
    "  [ 0.66854692,  0.79562378,  0.87755553,  0.92795967],\n",
    "  [ 0.97934501,  0.99144213,  0.99646691,  0.99854353]])\n",
    "\n",
    "print('next_h error: ', rel_error(expected_next_h, next_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RNN: step backward (4 Pts)\n",
    "In the `VanillaRNN` class in the file `lib/rnn/layer_utils.py` complete the `step_backward` function.\n",
    "\n",
    "After doing so run the following to numerically gradient check the implementation. You should see errors less than `1e-8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "dx error:  5.473855898318782e-10\ndprev_h error:  2.408484610600567e-10\ndWx error:  1.264200416692046e-10\ndWh error:  3.1863456358952086e-10\ndb error:  1.655233896888798e-11\n"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "np.random.seed(231)\n",
    "N, D, H = 4, 5, 6\n",
    "\n",
    "rnn = VanillaRNN(D, H, init_scale=0.02, name=\"rnn_test\")\n",
    "\n",
    "x = np.random.randn(N, D)\n",
    "h = np.random.randn(N, H)\n",
    "Wx = np.random.randn(D, H)\n",
    "Wh = np.random.randn(H, H)\n",
    "b = np.random.randn(H)\n",
    "\n",
    "rnn.params[rnn.wx_name] = Wx\n",
    "rnn.params[rnn.wh_name] = Wh\n",
    "rnn.params[rnn.b_name] = b\n",
    "\n",
    "out, meta = rnn.step_forward(x, h)\n",
    "\n",
    "dnext_h = np.random.randn(*out.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: rnn.step_forward(x, h)[0], x, dnext_h)\n",
    "dprev_h_num = eval_numerical_gradient_array(lambda h: rnn.step_forward(x, h)[0], h, dnext_h)\n",
    "dWx_num = eval_numerical_gradient_array(lambda Wx: rnn.step_forward(x, h)[0], Wx, dnext_h)\n",
    "dWh_num = eval_numerical_gradient_array(lambda Wh: rnn.step_forward(x, h)[0], Wh, dnext_h)\n",
    "db_num = eval_numerical_gradient_array(lambda b: rnn.step_forward(x, h)[0], b, dnext_h)\n",
    "\n",
    "dx, dprev_h, dWx, dWh, db = rnn.step_backward(dnext_h, meta)\n",
    "\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dprev_h error: ', rel_error(dprev_h_num, dprev_h))\n",
    "print('dWx error: ', rel_error(dWx_num, dWx))\n",
    "print('dWh error: ', rel_error(dWh_num, dWh))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RNN: forward (4 Pts)\n",
    "Now that you have  completed the forward and backward passes for a single timestep of a vanilla RNN, you will see how they are combined to implement a RNN that process an entire sequence of data.\n",
    "\n",
    "In the `VanillaRNN` class in the file `lib/rnn/layer_utils.py`, complete the function `forward`. This is implemented using the `step_forward` function that you defined above.\n",
    "\n",
    "After doing so run the following to check the implementation. You should see errors less than `1e-7`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "h error:  7.728466151011529e-08\n"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "N, T, D, H = 2, 3, 4, 5\n",
    "\n",
    "rnn = VanillaRNN(D, H, init_scale=0.02, name=\"rnn_test\")\n",
    "\n",
    "x = np.linspace(-0.1, 0.3, num=N*T*D).reshape(N, T, D)\n",
    "h0 = np.linspace(-0.3, 0.1, num=N*H).reshape(N, H)\n",
    "Wx = np.linspace(-0.2, 0.4, num=D*H).reshape(D, H)\n",
    "Wh = np.linspace(-0.4, 0.1, num=H*H).reshape(H, H)\n",
    "b = np.linspace(-0.7, 0.1, num=H)\n",
    "\n",
    "rnn.params[rnn.wx_name] = Wx\n",
    "rnn.params[rnn.wh_name] = Wh\n",
    "rnn.params[rnn.b_name] = b\n",
    "\n",
    "h = rnn.forward(x, h0)\n",
    "expected_h = np.asarray([\n",
    "  [\n",
    "    [-0.42070749, -0.27279261, -0.11074945,  0.05740409,  0.22236251],\n",
    "    [-0.39525808, -0.22554661, -0.0409454,   0.14649412,  0.32397316],\n",
    "    [-0.42305111, -0.24223728, -0.04287027,  0.15997045,  0.35014525],\n",
    "  ],\n",
    "  [\n",
    "    [-0.55857474, -0.39065825, -0.19198182,  0.02378408,  0.23735671],\n",
    "    [-0.27150199, -0.07088804,  0.13562939,  0.33099728,  0.50158768],\n",
    "    [-0.51014825, -0.30524429, -0.06755202,  0.17806392,  0.40333043]]])\n",
    "\n",
    "print('h error: ', rel_error(expected_h, h))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RNN: backward (4 Pts)\n",
    "In the file `lib/rnn/layer_utils.py`, complete the backward pass for a vanilla RNN in the function `backward` in the `VanillaRNN` class. This runs back-propagation over the entire sequence, calling into the `step_backward` function defined above.\n",
    "\n",
    "You should see errors less than 5e-7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "dx error:  2.2600059617453742e-08\ndh0 error:  8.272802161387549e-10\ndWx error:  2.077530726046649e-08\ndWh error:  1.5104222914511353e-08\ndb error:  3.7334931443820546e-10\n"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "np.random.seed(231)\n",
    "\n",
    "N, D, T, H = 2, 3, 10, 5\n",
    "\n",
    "rnn = VanillaRNN(D, H, init_scale=0.02, name=\"rnn_test\")\n",
    "\n",
    "x = np.random.randn(N, T, D)\n",
    "h0 = np.random.randn(N, H)\n",
    "Wx = np.random.randn(D, H)\n",
    "Wh = np.random.randn(H, H)\n",
    "b = np.random.randn(H)\n",
    "\n",
    "rnn.params[rnn.wx_name] = Wx\n",
    "rnn.params[rnn.wh_name] = Wh\n",
    "rnn.params[rnn.b_name] = b\n",
    "\n",
    "out = rnn.forward(x, h0)\n",
    "\n",
    "dout = np.random.randn(*out.shape)\n",
    "\n",
    "dx, dh0 = rnn.backward(dout)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: rnn.forward(x, h0), x, dout)\n",
    "dh0_num = eval_numerical_gradient_array(lambda h0: rnn.forward(x, h0), h0, dout)\n",
    "dWx_num = eval_numerical_gradient_array(lambda Wx: rnn.forward(x, h0), Wx, dout)\n",
    "dWh_num = eval_numerical_gradient_array(lambda Wh: rnn.forward(x, h0), Wh, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: rnn.forward(x, h0), b, dout)\n",
    "\n",
    "dWx = rnn.grads[rnn.wx_name]\n",
    "dWh = rnn.grads[rnn.wh_name]\n",
    "db = rnn.grads[rnn.b_name]\n",
    "\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dh0 error: ', rel_error(dh0_num, dh0))\n",
    "print('dWx error: ', rel_error(dWx_num, dWx))\n",
    "print('dWh error: ', rel_error(dWh_num, dWh))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding: forward (4 Pts)\n",
    "In deep learning systems, we commonly represent words using vectors. Each word of the vocabulary will be associated with a vector, and these vectors will be learned jointly with the rest of the system.\n",
    "\n",
    "In the file `lib/rnn/layer_utils.py`, implement the function `forward` in the `word_embedding` class to convert words (represented by integers) into vectors. Run the following to check the implementation. You should see error around `1e-8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "out error:  1.0000000094736443e-08\n"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "N, T, V, D = 2, 4, 5, 3\n",
    "\n",
    "we = word_embedding(V, D, name=\"we\")\n",
    "\n",
    "x = np.asarray([[0, 3, 1, 2], [2, 1, 0, 3]])\n",
    "W = np.linspace(0, 1, num=V*D).reshape(V, D)\n",
    "\n",
    "we.params[we.w_name] = W\n",
    "\n",
    "out = we.forward(x)\n",
    "expected_out = np.asarray([\n",
    " [[ 0.,          0.07142857,  0.14285714],\n",
    "  [ 0.64285714,  0.71428571,  0.78571429],\n",
    "  [ 0.21428571,  0.28571429,  0.35714286],\n",
    "  [ 0.42857143,  0.5,         0.57142857]],\n",
    " [[ 0.42857143,  0.5,         0.57142857],\n",
    "  [ 0.21428571,  0.28571429,  0.35714286],\n",
    "  [ 0.,          0.07142857,  0.14285714],\n",
    "  [ 0.64285714,  0.71428571,  0.78571429]]])\n",
    "\n",
    "print('out error: ', rel_error(expected_out, out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding: backward (4 Pts)\n",
    "Implement the backward pass for the word embedding function in the function `backward` in the `word_embedding` class. After doing so run the following to numerically gradient check your implementation. You should see errors less than `1e-11`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "dW error:  3.2759440934795915e-12\n"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "np.random.seed(231)\n",
    "\n",
    "N, T, V, D = 50, 3, 5, 6\n",
    "\n",
    "we = word_embedding(V, D, name=\"we\")\n",
    "\n",
    "x = np.random.randint(V, size=(N, T))\n",
    "W = np.random.randn(V, D)\n",
    "\n",
    "we.params[we.w_name] = W\n",
    "\n",
    "out = we.forward(x)\n",
    "dout = np.random.randn(*out.shape)\n",
    "we.backward(dout)\n",
    "\n",
    "dW = we.grads[we.w_name]\n",
    "\n",
    "f = lambda W: we.forward(x)\n",
    "dW_num = eval_numerical_gradient_array(f, W, dout)\n",
    "\n",
    "print('dW error: ', rel_error(dW, dW_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Fully Connected layer\n",
    "At every timestep we use an affine function to transform the RNN hidden vector at that timestep into scores for each word in the vocabulary. Because this is very similar to the fully connected layer that you implemented in assignment 1, we have provided this function for you in the `forward` and `backward` functions in the file `lib/rnn/layer_util.py`. Run the following to perform numeric gradient checking on the implementation. You should see errors less than 1e-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "dx error:  3.2269470390098687e-10\ndw error:  3.8595619942595054e-11\ndb error:  1.1455396263586309e-11\n"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "np.random.seed(231)\n",
    "\n",
    "# Gradient check for temporal affine layer\n",
    "N, T, D, M = 2, 3, 4, 5\n",
    "\n",
    "t_fc = temporal_fc(D, M, init_scale=0.02, name='test_t_fc')\n",
    "\n",
    "x = np.random.randn(N, T, D)\n",
    "w = np.random.randn(D, M)\n",
    "b = np.random.randn(M)\n",
    "\n",
    "t_fc.params[t_fc.w_name] = w\n",
    "t_fc.params[t_fc.b_name] = b\n",
    "\n",
    "out = t_fc.forward(x)\n",
    "\n",
    "dout = np.random.randn(*out.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: t_fc.forward(x), x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: t_fc.forward(x), w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: t_fc.forward(x), b, dout)\n",
    "\n",
    "dx = t_fc.backward(dout)\n",
    "dw = t_fc.grads[t_fc.w_name]\n",
    "db = t_fc.grads[t_fc.b_name]\n",
    "\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Softmax Cross-Entropy loss\n",
    "When rolling out a RNN language model to generate a sentence, at every timestep we produce a score for each word in the vocabulary, propotional to the predicted likelihood of this word appearing at the particular timestep in the sentence. We know the ground-truth word at each timestep, so we use a softmax cross-entropy loss function to (1) compute a proper probability distribution over the words in the vocabulary at every time step and (2) use this to compute loss and gradient at each timestep. We sum the losses over time and average them over the minibatch.\n",
    "\n",
    "We provide this loss function for you; look at the `temporal_softmax_CE_loss` function in the file `lib/rnn/layer_utils.py`.\n",
    "\n",
    "Run the following cell to sanity check the loss and perform numeric gradient checking on the function. You should see an error for dx less than 1e-7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2.3026547279318357\n23.026307039328714\n2.2989009292538665\ndx error:  4.0464746298031226e-08\n"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "loss_func = temporal_softmax_CE_loss()\n",
    "\n",
    "# Sanity check for temporal softmax loss\n",
    "N, T, V = 100, 1, 10\n",
    "\n",
    "def check_loss(N, T, V, p):\n",
    "    x = 0.001 * np.random.randn(N, T, V)\n",
    "    y = np.random.randint(V, size=(N, T))\n",
    "    mask = np.random.rand(N, T) <= p\n",
    "    print(loss_func.forward(x, y, mask))\n",
    "  \n",
    "check_loss(100, 1, 10, 1.0)   # Should be about 2.3\n",
    "check_loss(100, 10, 10, 1.0)  # Should be about 23\n",
    "check_loss(5000, 10, 10, 0.1) # Should be about 2.3\n",
    "\n",
    "# Gradient check for temporal softmax loss\n",
    "N, T, V = 7, 8, 9\n",
    "\n",
    "x = np.random.randn(N, T, V)\n",
    "y = np.random.randint(V, size=(N, T))\n",
    "mask = (np.random.rand(N, T) > 0.5)\n",
    "\n",
    "loss = loss_func.forward(x, y, mask)\n",
    "dx = loss_func.backward()\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: loss_func.forward(x, y, mask), x, verbose=False)\n",
    "\n",
    "print('dx error: ', rel_error(dx, dx_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN for language modeling\n",
    "Now that you have the necessary layers, you can combine them to build a language modeling model. Open the file `lib/rnn/rnn.py` and look at the `TestRNN` class.\n",
    "\n",
    "For now only check the forward and backward pass of the `TestRNN` model and ignore the `TODOs` in the constructor; you will implement these later. After doing so, run the following to check the forward and backward pass using a small test case; you should see error less than `1e-10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loss:  51.094918913361184\nexpected loss:  51.0949189134\ndifference:  3.881694965457427e-11\n"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "N, D, H = 10, 20, 40\n",
    "V = 4\n",
    "T = 13\n",
    "\n",
    "model = TestRNN(D, H, cell_type='rnn')\n",
    "loss_func = temporal_softmax_CE_loss()\n",
    "\n",
    "# Set all model parameters to fixed values\n",
    "for k, v in model.params.items():\n",
    "    model.params[k] = np.linspace(-1.4, 1.3, num=v.size).reshape(*v.shape)\n",
    "model.assign_params()\n",
    "\n",
    "features = np.linspace(-1.5, 0.3, num=(N * D * T)).reshape(N, T, D)\n",
    "h0 = np.linspace(-1.5, 0.5, num=(N*H)).reshape(N, H)\n",
    "labels = (np.arange(N * T) % V).reshape(N, T)\n",
    "\n",
    "pred = model.forward(features, h0)\n",
    "\n",
    "# You'll need this\n",
    "mask = np.ones((N, T))\n",
    "\n",
    "loss = loss_func.forward(pred, labels, mask)\n",
    "dLoss = loss_func.backward()\n",
    "\n",
    "expected_loss = 51.0949189134\n",
    "\n",
    "print('loss: ', loss)\n",
    "print('expected loss: ', expected_loss)\n",
    "print('difference: ', abs(loss - expected_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to perform more detailed gradient checking on the backward pass of the `TestRNN` class; you should errors around `1e-7` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "vanilla_rnn_b relative error: 9.451394e-08\nvanilla_rnn_wh relative error: 3.221744e-08\nvanilla_rnn_wx relative error: 9.508480e-08\n"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "np.random.seed(231)\n",
    "\n",
    "batch_size = 2\n",
    "timesteps = 3\n",
    "input_dim = 4\n",
    "hidden_dim = 6\n",
    "label_size = 4\n",
    "\n",
    "labels = np.random.randint(label_size, size=(batch_size, timesteps))\n",
    "features = np.random.randn(batch_size, timesteps, input_dim)\n",
    "h0 = np.random.randn(batch_size, hidden_dim)\n",
    "\n",
    "model = TestRNN(input_dim, hidden_dim, cell_type='rnn')\n",
    "loss_func = temporal_softmax_CE_loss()\n",
    "\n",
    "pred = model.forward(features, h0)\n",
    "\n",
    "# You'll need this\n",
    "mask = np.ones((batch_size, timesteps))\n",
    "\n",
    "loss = loss_func.forward(pred, labels, mask)\n",
    "dLoss = loss_func.backward()\n",
    "\n",
    "dout, dh0 = model.backward(dLoss)\n",
    "\n",
    "grads = model.grads\n",
    "\n",
    "for param_name in sorted(grads):\n",
    "    f = lambda _: loss_func.forward(model.forward(features, h0), labels, mask)\n",
    "    param_grad_num = eval_numerical_gradient(f, model.params[param_name], verbose=False, h=1e-6)\n",
    "    e = rel_error(param_grad_num, grads[param_name])\n",
    "    print('%s relative error: %e' % (param_name, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "Vanilla RNNs can be tough to train on long sequences due to vanishing and exploding gradiants. LSTMs solve this problem by replacing the simple update rule in the forward step of the vanilla RNN with a gating mechanism as follows.\n",
    "\n",
    "Similar to the vanilla RNN, at each timestep we receive an input $x_t\\in\\mathbb{R}^D$ and the previous hidden state $h_{t-1}\\in\\mathbb{R}^H$. Crucially, the LSTM also maintains an $H$-dimensional *cell state*, so we also receive the previous cell state $c_{t-1}\\in\\mathbb{R}^H$. The learnable parameters of the LSTM are an *input-to-hidden* matrix $W_x\\in\\mathbb{R}^{4H\\times D}$, a *hidden-to-hidden* matrix $W_h\\in\\mathbb{R}^{4H\\times H}$ and a *bias vector* $b\\in\\mathbb{R}^{4H}$.\n",
    "\n",
    "At each timestep we first compute an *activation vector* $a\\in\\mathbb{R}^{4H}$ as $a=W_xx_t + W_hh_{t-1}+b$. We then divide this into four vectors $a_i,a_f,a_o,a_g\\in\\mathbb{R}^H$ where $a_i$ consists of the first $H$ elements of $a$, $a_f$ is the next $H$ elements of $a$, etc. We then compute the *input gate* $g\\in\\mathbb{R}^H$, *forget gate* $f\\in\\mathbb{R}^H$, *output gate* $o\\in\\mathbb{R}^H$ and *gate gate* $g\\in\\mathbb{R}^H$ as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "i = \\sigma(a_i) \\hspace{2pc}\n",
    "f = \\sigma(a_f) \\hspace{2pc}\n",
    "o = \\sigma(a_o) \\hspace{2pc}\n",
    "g = \\tanh(a_g)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\sigma$ is the sigmoid function and $\\tanh$ is the hyperbolic tangent, both applied elementwise.\n",
    "\n",
    "Finally we compute the next cell state $c_t$ and next hidden state $h_t$ as\n",
    "\n",
    "$$\n",
    "c_{t} = f\\odot c_{t-1} + i\\odot g \\hspace{4pc}\n",
    "h_t = o\\odot\\tanh(c_t)\n",
    "$$\n",
    "\n",
    "where $\\odot$ is the elementwise product of vectors.\n",
    "\n",
    "In the rest of the notebook we will implement the LSTM update rule and apply it to the text generation task. \n",
    "\n",
    "In the code, we assume that data is stored in batches so that $X_t \\in \\mathbb{R}^{N\\times D}$, and will work with *transposed* versions of the parameters: $W_x \\in \\mathbb{R}^{D \\times 4H}$, $W_h \\in \\mathbb{R}^{H\\times 4H}$ so that activations $A \\in \\mathbb{R}^{N\\times 4H}$ can be computed efficiently as $A = X_t W_x + H_{t-1} W_h$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM: step forward (6 Pts)\n",
    "Implement the forward pass for a single timestep of an LSTM in the `step_forward` function in the file `lib/rnn/layer_utils.py`. This should be similar to the `step_forward` function that you implemented above, but using the LSTM update rule instead.\n",
    "\n",
    "Once you are done, run the following to perform a simple test of your implementation. You should see errors around `1e-8` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "next_h error:  5.7054131185818695e-09\nnext_c error:  5.8143123088804145e-09\n"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "N, D, H = 3, 4, 5\n",
    "\n",
    "lstm = LSTM(D, H, init_scale=0.02, name='test_lstm')\n",
    "\n",
    "x = np.linspace(-0.4, 1.2, num=N*D).reshape(N, D)\n",
    "prev_h = np.linspace(-0.3, 0.7, num=N*H).reshape(N, H)\n",
    "prev_c = np.linspace(-0.4, 0.9, num=N*H).reshape(N, H)\n",
    "Wx = np.linspace(-2.1, 1.3, num=4*D*H).reshape(D, 4 * H)\n",
    "Wh = np.linspace(-0.7, 2.2, num=4*H*H).reshape(H, 4 * H)\n",
    "b = np.linspace(0.3, 0.7, num=4*H)\n",
    "\n",
    "lstm.params[lstm.wx_name] = Wx\n",
    "lstm.params[lstm.wh_name] = Wh\n",
    "lstm.params[lstm.b_name] = b\n",
    "\n",
    "next_h, next_c, cache = lstm.step_forward(x, prev_h, prev_c)\n",
    "\n",
    "expected_next_h = np.asarray([\n",
    "    [ 0.24635157,  0.28610883,  0.32240467,  0.35525807,  0.38474904],\n",
    "    [ 0.49223563,  0.55611431,  0.61507696,  0.66844003,  0.7159181 ],\n",
    "    [ 0.56735664,  0.66310127,  0.74419266,  0.80889665,  0.858299  ]])\n",
    "expected_next_c = np.asarray([\n",
    "    [ 0.32986176,  0.39145139,  0.451556,    0.51014116,  0.56717407],\n",
    "    [ 0.66382255,  0.76674007,  0.87195994,  0.97902709,  1.08751345],\n",
    "    [ 0.74192008,  0.90592151,  1.07717006,  1.25120233,  1.42395676]])\n",
    "\n",
    "print('next_h error: ', rel_error(expected_next_h, next_h))\n",
    "print('next_c error: ', rel_error(expected_next_c, next_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM: step backward  (6 Pts)\n",
    "Implement the backward pass for a single LSTM timestep in the function `step_backward` in the file `lib/rnn/layer_utils.py`. Once you are done, run the following to perform numeric gradient checking on your implementation. You should see errors around `1e-6` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "dx error:  7.101740423193077e-10\ndh error:  1.2111007232199075e-08\ndc error:  1.0127281079074958e-08\ndWx error:  7.155327175373662e-08\ndWh error:  9.784434003770472e-08\ndb error:  1.8671697284523756e-08\n"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "np.random.seed(231)\n",
    "\n",
    "N, D, H = 4, 5, 6\n",
    "\n",
    "lstm = LSTM(D, H, init_scale=0.02, name='test_lstm')\n",
    "\n",
    "x = np.random.randn(N, D)\n",
    "prev_h = np.random.randn(N, H)\n",
    "prev_c = np.random.randn(N, H)\n",
    "Wx = np.random.randn(D, 4 * H)\n",
    "Wh = np.random.randn(H, 4 * H)\n",
    "b = np.random.randn(4 * H)\n",
    "\n",
    "lstm.params[lstm.wx_name] = Wx\n",
    "lstm.params[lstm.wh_name] = Wh\n",
    "lstm.params[lstm.b_name] = b\n",
    "\n",
    "next_h, next_c, cache = lstm.step_forward(x, prev_h, prev_c)\n",
    "\n",
    "dnext_h = np.random.randn(*next_h.shape)\n",
    "dnext_c = np.random.randn(*next_c.shape)\n",
    "\n",
    "fx_h = lambda x: lstm.step_forward(x, prev_h, prev_c)[0]\n",
    "fh_h = lambda h: lstm.step_forward(x, prev_h, prev_c)[0]\n",
    "fc_h = lambda c: lstm.step_forward(x, prev_h, prev_c)[0]\n",
    "fWx_h = lambda Wx: lstm.step_forward(x, prev_h, prev_c)[0]\n",
    "fWh_h = lambda Wh: lstm.step_forward(x, prev_h, prev_c)[0]\n",
    "fb_h = lambda b: lstm.step_forward(x, prev_h, prev_c)[0]\n",
    "\n",
    "fx_c = lambda x: lstm.step_forward(x, prev_h, prev_c)[1]\n",
    "fh_c = lambda h: lstm.step_forward(x, prev_h, prev_c)[1]\n",
    "fc_c = lambda c: lstm.step_forward(x, prev_h, prev_c)[1]\n",
    "fWx_c = lambda Wx: lstm.step_forward(x, prev_h, prev_c)[1]\n",
    "fWh_c = lambda Wh: lstm.step_forward(x, prev_h, prev_c)[1]\n",
    "fb_c = lambda b: lstm.step_forward(x, prev_h, prev_c)[1]\n",
    "\n",
    "num_grad = eval_numerical_gradient_array\n",
    "\n",
    "dx_num = num_grad(fx_h, x, dnext_h) + num_grad(fx_c, x, dnext_c)\n",
    "dh_num = num_grad(fh_h, prev_h, dnext_h) + num_grad(fh_c, prev_h, dnext_c)\n",
    "dc_num = num_grad(fc_h, prev_c, dnext_h) + num_grad(fc_c, prev_c, dnext_c)\n",
    "dWx_num = num_grad(fWx_h, Wx, dnext_h) + num_grad(fWx_c, Wx, dnext_c)\n",
    "dWh_num = num_grad(fWh_h, Wh, dnext_h) + num_grad(fWh_c, Wh, dnext_c)\n",
    "db_num = num_grad(fb_h, b, dnext_h) + num_grad(fb_c, b, dnext_c)\n",
    "\n",
    "dx, dh, dc, dWx, dWh, db = lstm.step_backward(dnext_h, dnext_c, cache)\n",
    "\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dh error: ', rel_error(dh_num, dh))\n",
    "print('dc error: ', rel_error(dc_num, dc))\n",
    "print('dWx error: ', rel_error(dWx_num, dWx))\n",
    "print('dWh error: ', rel_error(dWh_num, dWh))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM: forward (6 Pts)\n",
    "In the class `lstm` in the file `lib/rnn/layer_utils.py`, implement the `forward` function to run an LSTM forward on an entire timeseries of data.\n",
    "\n",
    "When you are done, run the following to check your implementation. You should see an error around `1e-7`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "h error:  8.610537452106624e-08\n"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "N, D, H, T = 2, 5, 4, 3\n",
    "\n",
    "lstm = LSTM(D, H, init_scale=0.02, name='test_lstm')\n",
    "\n",
    "x = np.linspace(-0.4, 0.6, num=N*T*D).reshape(N, T, D)\n",
    "h0 = np.linspace(-0.4, 0.8, num=N*H).reshape(N, H)\n",
    "Wx = np.linspace(-0.2, 0.9, num=4*D*H).reshape(D, 4 * H)\n",
    "Wh = np.linspace(-0.3, 0.6, num=4*H*H).reshape(H, 4 * H)\n",
    "b = np.linspace(0.2, 0.7, num=4*H)\n",
    "\n",
    "lstm.params[lstm.wx_name] = Wx\n",
    "lstm.params[lstm.wh_name] = Wh\n",
    "lstm.params[lstm.b_name] = b\n",
    "\n",
    "h = lstm.forward(x, h0)\n",
    "\n",
    "expected_h = np.asarray([\n",
    " [[ 0.01764008,  0.01823233,  0.01882671,  0.0194232 ],\n",
    "  [ 0.11287491,  0.12146228,  0.13018446,  0.13902939],\n",
    "  [ 0.31358768,  0.33338627,  0.35304453,  0.37250975]],\n",
    " [[ 0.45767879,  0.4761092,   0.4936887,   0.51041945],\n",
    "  [ 0.6704845,   0.69350089,  0.71486014,  0.7346449 ],\n",
    "  [ 0.81733511,  0.83677871,  0.85403753,  0.86935314]]])\n",
    "\n",
    "print('h error: ', rel_error(expected_h, h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM: backward (6 Pts)\n",
    "Implement the backward pass for an LSTM over an entire timeseries of data in the function `backward` in the `lstm` class in the file `lib/rnn/layer_utils.py`. When you are done, run the following to perform numeric gradient checking on your implementation. You should see errors around `1e-7` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "dx error:  2.0483953693630212e-09\ndh0 error:  2.7121606141117426e-10\ndWx error:  5.543955375943874e-09\ndWh error:  1.466240105052507e-07\ndb error:  6.543147221357796e-10\n"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "np.random.seed(231)\n",
    "\n",
    "N, D, T, H = 2, 3, 10, 6\n",
    "\n",
    "lstm = LSTM(D, H, init_scale=0.02, name='test_lstm')\n",
    "\n",
    "x = np.random.randn(N, T, D)\n",
    "h0 = np.random.randn(N, H)\n",
    "Wx = np.random.randn(D, 4 * H)\n",
    "Wh = np.random.randn(H, 4 * H)\n",
    "b = np.random.randn(4 * H)\n",
    "\n",
    "lstm.params[lstm.wx_name] = Wx\n",
    "lstm.params[lstm.wh_name] = Wh\n",
    "lstm.params[lstm.b_name] = b\n",
    "\n",
    "out = lstm.forward(x, h0)\n",
    "\n",
    "dout = np.random.randn(*out.shape)\n",
    "\n",
    "dx, dh0 = lstm.backward(dout)\n",
    "dWx = lstm.grads[lstm.wx_name] \n",
    "dWh = lstm.grads[lstm.wh_name]\n",
    "db = lstm.grads[lstm.b_name]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: lstm.forward(x, h0), x, dout)\n",
    "dh0_num = eval_numerical_gradient_array(lambda h0: lstm.forward(x, h0), h0, dout)\n",
    "dWx_num = eval_numerical_gradient_array(lambda Wx: lstm.forward(x, h0), Wx, dout)\n",
    "dWh_num = eval_numerical_gradient_array(lambda Wh: lstm.forward(x, h0), Wh, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: lstm.forward(x, h0), b, dout)\n",
    "\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dh0 error: ', rel_error(dh0_num, dh0))\n",
    "print('dWx error: ', rel_error(dWx_num, dWx))\n",
    "print('dWh error: ', rel_error(dWh_num, dWh))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model (2 Pts)\n",
    "\n",
    "Now that you have implemented an LSTM, update the initialization of the `TestRNN` class in the file `lib/rnn/rnn.py` to handle the case where `self.cell_type` is `lstm`. \n",
    "\n",
    "Once you have done so, run the following to check your implementation. You should see a difference of less than `1e-10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loss:  49.21402563544293\nexpected loss:  49.2140256354\ndifference:  4.293099209462525e-11\n"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "N, D, H = 10, 20, 40\n",
    "V = 4\n",
    "T = 13\n",
    "\n",
    "model = TestRNN(D, H, cell_type='lstm')\n",
    "loss_func = temporal_softmax_CE_loss()\n",
    "\n",
    "# Set all model parameters to fixed values\n",
    "for k, v in model.params.items():\n",
    "    model.params[k] = np.linspace(-1.4, 1.3, num=v.size).reshape(*v.shape)\n",
    "model.assign_params()\n",
    "\n",
    "features = np.linspace(-1.5, 0.3, num=(N * D * T)).reshape(N, T, D)\n",
    "h0 = np.linspace(-1.5, 0.5, num=(N*H)).reshape(N, H)\n",
    "labels = (np.arange(N * T) % V).reshape(N, T)\n",
    "\n",
    "pred = model.forward(features, h0)\n",
    "\n",
    "# You'll need this\n",
    "mask = np.ones((N, T))\n",
    "\n",
    "loss = loss_func.forward(pred, labels, mask)\n",
    "dLoss = loss_func.backward()\n",
    "\n",
    "expected_loss = 49.2140256354\n",
    "\n",
    "print('loss: ', loss)\n",
    "print('expected loss: ', expected_loss)\n",
    "print('difference: ', abs(loss - expected_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's have some fun!! (8 Pts)\n",
    "\n",
    "Now you have everything you need for language modeling. You will work on text generation using RNNs from any text source (novel, lyrics).\n",
    "\n",
    "The network is trained to predict what word is coming next given a previous word. Once you train the model, by looping the network, you can keep generating new text which is mimicing the original text source.\n",
    "\n",
    "We will use one of the most frequently downloaded e-books, Alice's Adventures in Wonderland, from Project Gutenberg, where the original link can be found [here](https://www.gutenberg.org/ebooks/11).\n",
    "\n",
    "For simplify training we extracted only the first chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "input_file = open(\"data/alice.txt\", \"r\")\n",
    "input_text = input_file.readlines()\n",
    "input_text = ''.join(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply run the following code to construct the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Input text size: 2170\nInput word number: 778\n"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "import re\n",
    "\n",
    "text = re.split(' |\\n',input_text.lower())  # all words are converted into lower case\n",
    "outputSize = len(text)\n",
    "word_list = list(set(text))\n",
    "dataSize = len(word_list)\n",
    "output = np.zeros(outputSize)\n",
    "for i in range(0, outputSize):\n",
    "    index = np.where(np.asarray(word_list) == text[i])\n",
    "    output[i] = index[0]\n",
    "data = output.astype(np.int)\n",
    "gt_labels = data[1:]\n",
    "input_data = data[:-1]\n",
    "\n",
    "print('Input text size: %s' % outputSize)\n",
    "print('Input word number: %s' % dataSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined a LanguageModelRNN class for you in `rnn.py`. Please fill in the TODO block in the constructor and complete the training loop.\n",
    "* In the constructor, design a recurrent neutral network consisting of a word_embedding layer, recurrent unit, and temporal fully connected layer so that they match the provided dimensions.\n",
    "* Please read the train.py under lib directory carefully and complete the TODO blocks in the train_net function.\n",
    "Then execute the following code block to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(Iteration 1 / 14400) loss: 332.8337839939059\nbest performance 3.5961272475795294%\n(Epoch 1 / 100) Training Accuracy: 0.035961272475795295\nbest performance 4.103273397879207%\n(Epoch 2 / 100) Training Accuracy: 0.04103273397879207\nbest performance 5.716920239741817%\n(Epoch 3 / 100) Training Accuracy: 0.05716920239741816\n(Iteration 501 / 14400) loss: 270.97780731229\nbest performance 6.77731673582296%\n(Epoch 4 / 100) Training Accuracy: 0.0677731673582296\nbest performance 7.514983863531581%\n(Epoch 5 / 100) Training Accuracy: 0.07514983863531581\nbest performance 9.497464269248502%\n(Epoch 6 / 100) Training Accuracy: 0.09497464269248501\n(Iteration 1001 / 14400) loss: 237.40790373650555\nbest performance 10.419548178884279%\n(Epoch 7 / 100) Training Accuracy: 0.10419548178884279\nbest performance 11.387736284001845%\n(Epoch 8 / 100) Training Accuracy: 0.11387736284001844\nbest performance 12.770862148455508%\n(Epoch 9 / 100) Training Accuracy: 0.12770862148455508\nbest performance 14.568925772245276%\n(Epoch 10 / 100) Training Accuracy: 0.14568925772245275\n(Iteration 1501 / 14400) loss: 208.0481946162364\nbest performance 17.10465652374366%\n(Epoch 11 / 100) Training Accuracy: 0.1710465652374366\nbest performance 19.64038727524205%\n(Epoch 12 / 100) Training Accuracy: 0.19640387275242047\nbest performance 21.853388658367912%\n(Epoch 13 / 100) Training Accuracy: 0.21853388658367912\n(Iteration 2001 / 14400) loss: 181.28586518000836\nbest performance 24.343015214384508%\n(Epoch 14 / 100) Training Accuracy: 0.24343015214384509\nbest performance 27.616413093591518%\n(Epoch 15 / 100) Training Accuracy: 0.2761641309359152\nbest performance 29.87551867219917%\n(Epoch 16 / 100) Training Accuracy: 0.2987551867219917\nbest performance 32.59566620562471%\n(Epoch 17 / 100) Training Accuracy: 0.3259566620562471\n(Iteration 2501 / 14400) loss: 162.43298312567597\nbest performance 34.80866758875057%\n(Epoch 18 / 100) Training Accuracy: 0.34808667588750575\nbest performance 36.83725218994929%\n(Epoch 19 / 100) Training Accuracy: 0.3683725218994929\nbest performance 38.91194098662979%\n(Epoch 20 / 100) Training Accuracy: 0.38911940986629784\n(Iteration 3001 / 14400) loss: 138.98426384540363\nbest performance 40.710004610419546%\n(Epoch 21 / 100) Training Accuracy: 0.4071000461041955\nbest performance 43.291839557399726%\n(Epoch 22 / 100) Training Accuracy: 0.43291839557399725\nbest performance 44.997694790225914%\n(Epoch 23 / 100) Training Accuracy: 0.4499769479022591\nbest performance 46.88796680497925%\n(Epoch 24 / 100) Training Accuracy: 0.46887966804979253\n(Iteration 3501 / 14400) loss: 128.83155854769907\nbest performance 48.91655140617796%\n(Epoch 25 / 100) Training Accuracy: 0.4891655140617796\nbest performance 50.8068234209313%\n(Epoch 26 / 100) Training Accuracy: 0.508068234209313\nbest performance 52.558782849239286%\n(Epoch 27 / 100) Training Accuracy: 0.5255878284923928\n(Iteration 4001 / 14400) loss: 109.43403132991679\nbest performance 54.58736745043799%\n(Epoch 28 / 100) Training Accuracy: 0.5458736745043798\nbest performance 55.7860765329645%\n(Epoch 29 / 100) Training Accuracy: 0.557860765329645\nbest performance 56.75426463808206%\n(Epoch 30 / 100) Training Accuracy: 0.5675426463808206\nbest performance 57.722452743199625%\n(Epoch 31 / 100) Training Accuracy: 0.5772245274319963\n(Iteration 4501 / 14400) loss: 99.39571928397882\nbest performance 58.82895343476257%\n(Epoch 32 / 100) Training Accuracy: 0.5882895343476257\nbest performance 60.07376671277086%\n(Epoch 33 / 100) Training Accuracy: 0.6007376671277086\nbest performance 60.7192254495159%\n(Epoch 34 / 100) Training Accuracy: 0.607192254495159\n(Iteration 5001 / 14400) loss: 90.18701665222673\nbest performance 61.917934532042416%\n(Epoch 35 / 100) Training Accuracy: 0.6191793453204242\nbest performance 62.56339326878746%\n(Epoch 36 / 100) Training Accuracy: 0.6256339326878746\nbest performance 62.88612263715998%\n(Epoch 37 / 100) Training Accuracy: 0.6288612263715998\nbest performance 63.16274781005071%\n(Epoch 38 / 100) Training Accuracy: 0.6316274781005071\n(Iteration 5501 / 14400) loss: 81.43757378963129\nbest performance 63.57768556938681%\n(Epoch 39 / 100) Training Accuracy: 0.6357768556938681\nbest performance 64.40756108805901%\n(Epoch 40 / 100) Training Accuracy: 0.6440756108805902\nbest performance 65.05301982480405%\n(Epoch 41 / 100) Training Accuracy: 0.6505301982480406\n(Iteration 6001 / 14400) loss: 78.7228926869929\nbest performance 65.51406177962194%\n(Epoch 42 / 100) Training Accuracy: 0.6551406177962195\nbest performance 66.15952051636698%\n(Epoch 43 / 100) Training Accuracy: 0.6615952051636699\nbest performance 66.57445827570308%\n(Epoch 44 / 100) Training Accuracy: 0.6657445827570309\nbest performance 66.85108344859381%\n(Epoch 45 / 100) Training Accuracy: 0.6685108344859382\n(Iteration 6501 / 14400) loss: 76.43677914775363\nbest performance 67.49654218533887%\n(Epoch 46 / 100) Training Accuracy: 0.6749654218533887\nbest performance 67.91147994467497%\n(Epoch 47 / 100) Training Accuracy: 0.6791147994467497\nbest performance 68.41862609497464%\n(Epoch 48 / 100) Training Accuracy: 0.6841862609497464\n(Iteration 7001 / 14400) loss: 69.4800236701717\nbest performance 68.55693868142001%\n(Epoch 49 / 100) Training Accuracy: 0.6855693868142001\nbest performance 68.9718764407561%\n(Epoch 50 / 100) Training Accuracy: 0.6897187644075611\nbest performance 69.3868142000922%\n(Epoch 51 / 100) Training Accuracy: 0.6938681420009221\nbest performance 69.75564776394651%\n(Epoch 52 / 100) Training Accuracy: 0.6975564776394652\n(Iteration 7501 / 14400) loss: 61.558016160869215\nbest performance 70.2166897187644%\n(Epoch 53 / 100) Training Accuracy: 0.702166897187644\nbest performance 70.76994006454588%\n(Epoch 54 / 100) Training Accuracy: 0.7076994006454588\nbest performance 70.90825265099124%\n(Epoch 55 / 100) Training Accuracy: 0.7090825265099124\n(Iteration 8001 / 14400) loss: 57.380929320025686\nbest performance 71.23098201936376%\n(Epoch 56 / 100) Training Accuracy: 0.7123098201936376\nbest performance 71.64591977869986%\n(Epoch 57 / 100) Training Accuracy: 0.7164591977869986\nbest performance 72.01475334255417%\n(Epoch 58 / 100) Training Accuracy: 0.7201475334255417\nbest performance 72.47579529737206%\n(Epoch 59 / 100) Training Accuracy: 0.7247579529737206\n(Iteration 8501 / 14400) loss: 55.33280849610326\nbest performance 72.66021207929921%\n(Epoch 60 / 100) Training Accuracy: 0.7266021207929921\nbest performance 72.706316274781%\n(Epoch 61 / 100) Training Accuracy: 0.72706316274781\nbest performance 73.44398340248964%\n(Epoch 62 / 100) Training Accuracy: 0.7344398340248963\n(Iteration 9001 / 14400) loss: 55.897930856762905\nbest performance 73.582295988935%\n(Epoch 63 / 100) Training Accuracy: 0.73582295988935\nbest performance 74.04333794375289%\n(Epoch 64 / 100) Training Accuracy: 0.7404333794375288\n(Epoch 65 / 100) Training Accuracy: 0.7404333794375288\n(Iteration 9501 / 14400) loss: 54.380788627076676\nbest performance 74.18165053019825%\n(Epoch 66 / 100) Training Accuracy: 0.7418165053019825\nbest performance 74.59658828953435%\n(Epoch 67 / 100) Training Accuracy: 0.7459658828953435\n(Epoch 68 / 100) Training Accuracy: 0.7450437989857077\nbest performance 74.68879668049793%\n(Epoch 69 / 100) Training Accuracy: 0.7468879668049793\n(Iteration 10001 / 14400) loss: 49.601951596118056\nbest performance 74.96542185338866%\n(Epoch 70 / 100) Training Accuracy: 0.7496542185338866\nbest performance 75.6108805901337%\n(Epoch 71 / 100) Training Accuracy: 0.756108805901337\nbest performance 75.74919317657907%\n(Epoch 72 / 100) Training Accuracy: 0.7574919317657907\n(Iteration 10501 / 14400) loss: 52.718869702483495\nbest performance 75.88750576302444%\n(Epoch 73 / 100) Training Accuracy: 0.7588750576302443\nbest performance 76.34854771784232%\n(Epoch 74 / 100) Training Accuracy: 0.7634854771784232\nbest performance 76.39465191332411%\n(Epoch 75 / 100) Training Accuracy: 0.7639465191332411\nbest performance 77.04011065006917%\n(Epoch 76 / 100) Training Accuracy: 0.7704011065006916\n(Iteration 11001 / 14400) loss: 44.09828773628204\n(Epoch 77 / 100) Training Accuracy: 0.7694790225910558\nbest performance 77.22452743199632%\n(Epoch 78 / 100) Training Accuracy: 0.7722452743199631\nbest performance 77.40894421392348%\n(Epoch 79 / 100) Training Accuracy: 0.7740894421392347\n(Iteration 11501 / 14400) loss: 46.06412038794181\nbest performance 77.59336099585063%\n(Epoch 80 / 100) Training Accuracy: 0.7759336099585062\nbest performance 77.91609036422315%\n(Epoch 81 / 100) Training Accuracy: 0.7791609036422315\nbest performance 77.96219455970494%\n(Epoch 82 / 100) Training Accuracy: 0.7796219455970493\nbest performance 78.19271553711388%\n(Epoch 83 / 100) Training Accuracy: 0.7819271553711388\n(Iteration 12001 / 14400) loss: 43.79906685525622\nbest performance 78.33102812355925%\n(Epoch 84 / 100) Training Accuracy: 0.7833102812355924\nbest performance 78.46934071000462%\n(Epoch 85 / 100) Training Accuracy: 0.7846934071000461\nbest performance 78.5154449054864%\n(Epoch 86 / 100) Training Accuracy: 0.785154449054864\n(Iteration 12501 / 14400) loss: 41.57351258875636\nbest performance 78.65375749193177%\n(Epoch 87 / 100) Training Accuracy: 0.7865375749193176\nbest performance 79.06869525126787%\n(Epoch 88 / 100) Training Accuracy: 0.7906869525126786\n(Epoch 89 / 100) Training Accuracy: 0.789303826648225\nbest performance 79.20700783771323%\n(Epoch 90 / 100) Training Accuracy: 0.7920700783771323\n(Iteration 13001 / 14400) loss: 39.27399198635497\nbest performance 79.29921622867681%\n(Epoch 91 / 100) Training Accuracy: 0.792992162286768\nbest performance 79.43752881512218%\n(Epoch 92 / 100) Training Accuracy: 0.7943752881512217\nbest performance 79.62194559704933%\n(Epoch 93 / 100) Training Accuracy: 0.7962194559704933\n(Iteration 13501 / 14400) loss: 40.14839515094544\nbest performance 79.80636237897649%\n(Epoch 94 / 100) Training Accuracy: 0.7980636237897649\nbest performance 79.89857076994006%\n(Epoch 95 / 100) Training Accuracy: 0.7989857076994007\nbest performance 80.1751959428308%\n(Epoch 96 / 100) Training Accuracy: 0.801751959428308\nbest performance 80.22130013831259%\n(Epoch 97 / 100) Training Accuracy: 0.8022130013831259\n(Iteration 14001 / 14400) loss: 42.43331567004431\n(Epoch 98 / 100) Training Accuracy: 0.8012909174734901\nbest performance 80.5440295066851%\n(Epoch 99 / 100) Training Accuracy: 0.8054402950668511\n(Epoch 100 / 100) Training Accuracy: 0.8045182111572153\n"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "# you can change the following parameters.\n",
    "D = 25  # input dimension\n",
    "H = 35  # hidden space dimension\n",
    "T = 50  # timesteps\n",
    "N = 15  # batch size\n",
    "max_epoch = 50  # max epoch size\n",
    "\n",
    "loss_func = temporal_softmax_CE_loss()\n",
    "# you can change the cell_type between 'rnn' and 'lstm'.\n",
    "model = LanguageModelRNN(dataSize, D, H, cell_type='lstm')\n",
    "optimizer = Adam(model, 5e-4)\n",
    "\n",
    "data = {'data_train': input_data, 'labels_train': gt_labels}\n",
    "\n",
    "results = train_net(data, model, loss_func, optimizer, timesteps=T, batch_size=N, max_epochs=max_epoch, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply run the following code block to check the loss and accuracy curve. (We expect training accuracy to be >80%, you can change all parameters above except `T` to try to improve your training performance. The higher your performance, the better your text samples below will get.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "&lt;Figure size 1080x864 with 2 Axes&gt;",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"712.194375pt\" version=\"1.1\" viewBox=\"0 0 877.4875 712.194375\" width=\"877.4875pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2020-10-06T22:23:03.232412</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 712.194375 \nL 877.4875 712.194375 \nL 877.4875 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 33.2875 318.827216 \nL 870.2875 318.827216 \nL 870.2875 22.318125 \nL 33.2875 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mb103619382\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"71.332955\" xlink:href=\"#mb103619382\" y=\"318.827216\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(68.151705 333.425653)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"177.753806\" xlink:href=\"#mb103619382\" y=\"318.827216\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <g transform=\"translate(171.391306 333.425653)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"284.174658\" xlink:href=\"#mb103619382\" y=\"318.827216\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <g transform=\"translate(277.812158 333.425653)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"390.59551\" xlink:href=\"#mb103619382\" y=\"318.827216\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <g transform=\"translate(384.23301 333.425653)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"497.016362\" xlink:href=\"#mb103619382\" y=\"318.827216\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <g transform=\"translate(490.653862 333.425653)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"603.437214\" xlink:href=\"#mb103619382\" y=\"318.827216\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <g transform=\"translate(593.893464 333.425653)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"709.858066\" xlink:href=\"#mb103619382\" y=\"318.827216\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 120 -->\n      <g transform=\"translate(700.314316 333.425653)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"816.278918\" xlink:href=\"#mb103619382\" y=\"318.827216\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 140 -->\n      <g transform=\"translate(806.735168 333.425653)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_9\">\n     <!-- Iteration -->\n     <g transform=\"translate(430.578906 347.103778)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-73\"/>\n       <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"29.492188\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"68.701172\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"130.224609\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"171.337891\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"232.617188\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"271.826172\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"299.609375\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"360.791016\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m7d6dba1f1f\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m7d6dba1f1f\" y=\"292.854057\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 50 -->\n      <g transform=\"translate(13.5625 296.653276)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m7d6dba1f1f\" y=\"247.736285\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 251.535504)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m7d6dba1f1f\" y=\"202.618514\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 150 -->\n      <g transform=\"translate(7.2 206.417733)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m7d6dba1f1f\" y=\"157.500743\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 200 -->\n      <g transform=\"translate(7.2 161.299962)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m7d6dba1f1f\" y=\"112.382972\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 250 -->\n      <g transform=\"translate(7.2 116.18219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m7d6dba1f1f\" y=\"67.2652\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 300 -->\n      <g transform=\"translate(7.2 71.064419)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#pb954fbb882)\" d=\"M 71.332955 37.684676 \nL 76.653997 35.795811 \nL 81.97504 79.638186 \nL 87.296082 81.0271 \nL 92.617125 88.870268 \nL 97.938168 89.847616 \nL 103.25921 99.584134 \nL 108.580253 100.8224 \nL 113.901295 106.338224 \nL 119.222338 113.911374 \nL 124.54338 123.594477 \nL 129.864423 122.030503 \nL 135.185466 133.002956 \nL 140.506508 131.876289 \nL 145.827551 147.194565 \nL 151.148593 148.917452 \nL 156.469636 156.590199 \nL 161.790679 156.903341 \nL 167.111721 163.480154 \nL 172.432764 165.687031 \nL 177.753806 171.70833 \nL 183.074849 175.618498 \nL 188.395892 182.797276 \nL 193.716934 178.416326 \nL 199.037977 192.454933 \nL 204.359019 194.102576 \nL 209.680062 190.245949 \nL 215.001105 202.207001 \nL 220.322147 204.436221 \nL 225.64319 207.355096 \nL 230.964232 211.51148 \nL 236.285275 212.535182 \nL 241.606318 218.00845 \nL 246.92736 220.445469 \nL 252.248403 219.739972 \nL 257.569445 225.625305 \nL 262.890488 227.687335 \nL 268.211531 228.438384 \nL 273.532573 236.541384 \nL 278.853616 239.581122 \nL 284.174658 241.060721 \nL 289.495701 238.656435 \nL 294.816743 245.331184 \nL 300.137786 242.957775 \nL 305.458829 249.696531 \nL 316.100914 250.652366 \nL 321.421956 249.391295 \nL 326.742999 253.645475 \nL 332.064042 253.507297 \nL 337.385084 255.371661 \nL 342.706127 259.403787 \nL 348.027169 259.980466 \nL 353.348212 259.529111 \nL 358.669255 261.379446 \nL 363.990297 262.282434 \nL 369.31134 266.163225 \nL 374.632382 265.618358 \nL 379.953425 264.538049 \nL 390.59551 268.494069 \nL 395.916553 266.503831 \nL 401.237595 268.180845 \nL 406.558638 271.055545 \nL 411.879681 270.233544 \nL 417.200723 272.141217 \nL 422.521766 271.448348 \nL 427.842808 275.850092 \nL 433.163851 276.54521 \nL 438.484894 277.849006 \nL 443.805936 278.769616 \nL 449.126979 277.761357 \nL 454.448021 282.728731 \nL 459.769064 279.354847 \nL 465.090106 279.766527 \nL 470.411149 282.665021 \nL 475.732192 283.006339 \nL 481.053234 277.707498 \nL 486.374277 282.916464 \nL 491.695319 282.492086 \nL 497.016362 281.820269 \nL 502.337405 281.031407 \nL 507.658447 285.757908 \nL 512.97949 288.029533 \nL 518.300532 289.483819 \nL 523.621575 285.176625 \nL 528.942618 289.124066 \nL 534.26366 287.125269 \nL 539.584703 286.409399 \nL 544.905745 291.757074 \nL 550.226788 287.877275 \nL 555.547831 290.993131 \nL 560.868873 286.941214 \nL 566.189916 288.639391 \nL 571.510958 288.562414 \nL 576.832001 291.957642 \nL 582.153044 290.936966 \nL 587.474086 293.877439 \nL 592.795129 292.285788 \nL 598.116171 294.098026 \nL 603.437214 291.323212 \nL 608.758257 293.25629 \nL 614.079299 290.271353 \nL 619.400342 293.703243 \nL 624.721384 296.658022 \nL 630.042427 295.054884 \nL 635.363469 296.470311 \nL 640.684512 292.329853 \nL 646.005555 297.919032 \nL 651.326597 291.147233 \nL 656.64764 296.130338 \nL 661.968682 293.226517 \nL 667.289725 296.794511 \nL 672.610768 297.352509 \nL 677.93181 298.146338 \nL 683.252853 297.123533 \nL 688.573895 298.746413 \nL 693.894938 293.523781 \nL 699.215981 296.597848 \nL 704.537023 298.343332 \nL 709.858066 299.374985 \nL 715.179108 295.070629 \nL 720.500151 298.899347 \nL 725.821194 300.398366 \nL 731.142236 297.07783 \nL 736.463279 296.64942 \nL 741.784321 301.895287 \nL 747.105364 298.065146 \nL 752.426407 302.748631 \nL 757.747449 300.485531 \nL 763.068492 299.649171 \nL 768.389534 298.974093 \nL 773.710577 301.180939 \nL 779.03162 300.418488 \nL 784.352662 300.498206 \nL 789.673705 301.860764 \nL 794.994747 298.364018 \nL 800.31579 300.716272 \nL 805.636832 302.748196 \nL 810.957875 305.34953 \nL 816.278918 302.267191 \nL 821.59996 301.675758 \nL 826.921003 301.833883 \nL 832.242045 301.433332 \nL 832.242045 301.433332 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <defs>\n     <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"ma82c802c45\" style=\"stroke:#1f77b4;\"/>\n    </defs>\n    <g clip-path=\"url(#pb954fbb882)\">\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"71.332955\" xlink:href=\"#ma82c802c45\" y=\"37.684676\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"76.653997\" xlink:href=\"#ma82c802c45\" y=\"35.795811\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"81.97504\" xlink:href=\"#ma82c802c45\" y=\"79.638186\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"87.296082\" xlink:href=\"#ma82c802c45\" y=\"81.0271\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"92.617125\" xlink:href=\"#ma82c802c45\" y=\"88.870268\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"97.938168\" xlink:href=\"#ma82c802c45\" y=\"89.847616\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"103.25921\" xlink:href=\"#ma82c802c45\" y=\"99.584134\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"108.580253\" xlink:href=\"#ma82c802c45\" y=\"100.8224\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"113.901295\" xlink:href=\"#ma82c802c45\" y=\"106.338224\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"119.222338\" xlink:href=\"#ma82c802c45\" y=\"113.911374\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"124.54338\" xlink:href=\"#ma82c802c45\" y=\"123.594477\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"129.864423\" xlink:href=\"#ma82c802c45\" y=\"122.030503\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"135.185466\" xlink:href=\"#ma82c802c45\" y=\"133.002956\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"140.506508\" xlink:href=\"#ma82c802c45\" y=\"131.876289\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"145.827551\" xlink:href=\"#ma82c802c45\" y=\"147.194565\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"151.148593\" xlink:href=\"#ma82c802c45\" y=\"148.917452\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"156.469636\" xlink:href=\"#ma82c802c45\" y=\"156.590199\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"161.790679\" xlink:href=\"#ma82c802c45\" y=\"156.903341\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"167.111721\" xlink:href=\"#ma82c802c45\" y=\"163.480154\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"172.432764\" xlink:href=\"#ma82c802c45\" y=\"165.687031\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"177.753806\" xlink:href=\"#ma82c802c45\" y=\"171.70833\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"183.074849\" xlink:href=\"#ma82c802c45\" y=\"175.618498\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"188.395892\" xlink:href=\"#ma82c802c45\" y=\"182.797276\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"193.716934\" xlink:href=\"#ma82c802c45\" y=\"178.416326\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"199.037977\" xlink:href=\"#ma82c802c45\" y=\"192.454933\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"204.359019\" xlink:href=\"#ma82c802c45\" y=\"194.102576\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"209.680062\" xlink:href=\"#ma82c802c45\" y=\"190.245949\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"215.001105\" xlink:href=\"#ma82c802c45\" y=\"202.207001\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"220.322147\" xlink:href=\"#ma82c802c45\" y=\"204.436221\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"225.64319\" xlink:href=\"#ma82c802c45\" y=\"207.355096\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"230.964232\" xlink:href=\"#ma82c802c45\" y=\"211.51148\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"236.285275\" xlink:href=\"#ma82c802c45\" y=\"212.535182\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"241.606318\" xlink:href=\"#ma82c802c45\" y=\"218.00845\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"246.92736\" xlink:href=\"#ma82c802c45\" y=\"220.445469\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"252.248403\" xlink:href=\"#ma82c802c45\" y=\"219.739972\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"257.569445\" xlink:href=\"#ma82c802c45\" y=\"225.625305\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"262.890488\" xlink:href=\"#ma82c802c45\" y=\"227.687335\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"268.211531\" xlink:href=\"#ma82c802c45\" y=\"228.438384\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"273.532573\" xlink:href=\"#ma82c802c45\" y=\"236.541384\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"278.853616\" xlink:href=\"#ma82c802c45\" y=\"239.581122\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"284.174658\" xlink:href=\"#ma82c802c45\" y=\"241.060721\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"289.495701\" xlink:href=\"#ma82c802c45\" y=\"238.656435\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"294.816743\" xlink:href=\"#ma82c802c45\" y=\"245.331184\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"300.137786\" xlink:href=\"#ma82c802c45\" y=\"242.957775\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"305.458829\" xlink:href=\"#ma82c802c45\" y=\"249.696531\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"310.779871\" xlink:href=\"#ma82c802c45\" y=\"250.157489\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"316.100914\" xlink:href=\"#ma82c802c45\" y=\"250.652366\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"321.421956\" xlink:href=\"#ma82c802c45\" y=\"249.391295\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"326.742999\" xlink:href=\"#ma82c802c45\" y=\"253.645475\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"332.064042\" xlink:href=\"#ma82c802c45\" y=\"253.507297\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"337.385084\" xlink:href=\"#ma82c802c45\" y=\"255.371661\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"342.706127\" xlink:href=\"#ma82c802c45\" y=\"259.403787\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"348.027169\" xlink:href=\"#ma82c802c45\" y=\"259.980466\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"353.348212\" xlink:href=\"#ma82c802c45\" y=\"259.529111\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"358.669255\" xlink:href=\"#ma82c802c45\" y=\"261.379446\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"363.990297\" xlink:href=\"#ma82c802c45\" y=\"262.282434\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"369.31134\" xlink:href=\"#ma82c802c45\" y=\"266.163225\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"374.632382\" xlink:href=\"#ma82c802c45\" y=\"265.618358\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"379.953425\" xlink:href=\"#ma82c802c45\" y=\"264.538049\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"385.274468\" xlink:href=\"#ma82c802c45\" y=\"266.562945\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"390.59551\" xlink:href=\"#ma82c802c45\" y=\"268.494069\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"395.916553\" xlink:href=\"#ma82c802c45\" y=\"266.503831\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"401.237595\" xlink:href=\"#ma82c802c45\" y=\"268.180845\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"406.558638\" xlink:href=\"#ma82c802c45\" y=\"271.055545\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"411.879681\" xlink:href=\"#ma82c802c45\" y=\"270.233544\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"417.200723\" xlink:href=\"#ma82c802c45\" y=\"272.141217\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"422.521766\" xlink:href=\"#ma82c802c45\" y=\"271.448348\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"427.842808\" xlink:href=\"#ma82c802c45\" y=\"275.850092\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"433.163851\" xlink:href=\"#ma82c802c45\" y=\"276.54521\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"438.484894\" xlink:href=\"#ma82c802c45\" y=\"277.849006\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"443.805936\" xlink:href=\"#ma82c802c45\" y=\"278.769616\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"449.126979\" xlink:href=\"#ma82c802c45\" y=\"277.761357\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"454.448021\" xlink:href=\"#ma82c802c45\" y=\"282.728731\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"459.769064\" xlink:href=\"#ma82c802c45\" y=\"279.354847\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"465.090106\" xlink:href=\"#ma82c802c45\" y=\"279.766527\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"470.411149\" xlink:href=\"#ma82c802c45\" y=\"282.665021\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"475.732192\" xlink:href=\"#ma82c802c45\" y=\"283.006339\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"481.053234\" xlink:href=\"#ma82c802c45\" y=\"277.707498\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"486.374277\" xlink:href=\"#ma82c802c45\" y=\"282.916464\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"491.695319\" xlink:href=\"#ma82c802c45\" y=\"282.492086\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"497.016362\" xlink:href=\"#ma82c802c45\" y=\"281.820269\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"502.337405\" xlink:href=\"#ma82c802c45\" y=\"281.031407\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"507.658447\" xlink:href=\"#ma82c802c45\" y=\"285.757908\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"512.97949\" xlink:href=\"#ma82c802c45\" y=\"288.029533\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"518.300532\" xlink:href=\"#ma82c802c45\" y=\"289.483819\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"523.621575\" xlink:href=\"#ma82c802c45\" y=\"285.176625\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"528.942618\" xlink:href=\"#ma82c802c45\" y=\"289.124066\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"534.26366\" xlink:href=\"#ma82c802c45\" y=\"287.125269\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"539.584703\" xlink:href=\"#ma82c802c45\" y=\"286.409399\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"544.905745\" xlink:href=\"#ma82c802c45\" y=\"291.757074\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"550.226788\" xlink:href=\"#ma82c802c45\" y=\"287.877275\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"555.547831\" xlink:href=\"#ma82c802c45\" y=\"290.993131\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"560.868873\" xlink:href=\"#ma82c802c45\" y=\"286.941214\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"566.189916\" xlink:href=\"#ma82c802c45\" y=\"288.639391\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"571.510958\" xlink:href=\"#ma82c802c45\" y=\"288.562414\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"576.832001\" xlink:href=\"#ma82c802c45\" y=\"291.957642\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"582.153044\" xlink:href=\"#ma82c802c45\" y=\"290.936966\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"587.474086\" xlink:href=\"#ma82c802c45\" y=\"293.877439\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"592.795129\" xlink:href=\"#ma82c802c45\" y=\"292.285788\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"598.116171\" xlink:href=\"#ma82c802c45\" y=\"294.098026\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"603.437214\" xlink:href=\"#ma82c802c45\" y=\"291.323212\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"608.758257\" xlink:href=\"#ma82c802c45\" y=\"293.25629\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"614.079299\" xlink:href=\"#ma82c802c45\" y=\"290.271353\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"619.400342\" xlink:href=\"#ma82c802c45\" y=\"293.703243\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"624.721384\" xlink:href=\"#ma82c802c45\" y=\"296.658022\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"630.042427\" xlink:href=\"#ma82c802c45\" y=\"295.054884\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"635.363469\" xlink:href=\"#ma82c802c45\" y=\"296.470311\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"640.684512\" xlink:href=\"#ma82c802c45\" y=\"292.329853\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"646.005555\" xlink:href=\"#ma82c802c45\" y=\"297.919032\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"651.326597\" xlink:href=\"#ma82c802c45\" y=\"291.147233\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"656.64764\" xlink:href=\"#ma82c802c45\" y=\"296.130338\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"661.968682\" xlink:href=\"#ma82c802c45\" y=\"293.226517\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"667.289725\" xlink:href=\"#ma82c802c45\" y=\"296.794511\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"672.610768\" xlink:href=\"#ma82c802c45\" y=\"297.352509\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"677.93181\" xlink:href=\"#ma82c802c45\" y=\"298.146338\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"683.252853\" xlink:href=\"#ma82c802c45\" y=\"297.123533\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"688.573895\" xlink:href=\"#ma82c802c45\" y=\"298.746413\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"693.894938\" xlink:href=\"#ma82c802c45\" y=\"293.523781\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"699.215981\" xlink:href=\"#ma82c802c45\" y=\"296.597848\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"704.537023\" xlink:href=\"#ma82c802c45\" y=\"298.343332\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"709.858066\" xlink:href=\"#ma82c802c45\" y=\"299.374985\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"715.179108\" xlink:href=\"#ma82c802c45\" y=\"295.070629\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"720.500151\" xlink:href=\"#ma82c802c45\" y=\"298.899347\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"725.821194\" xlink:href=\"#ma82c802c45\" y=\"300.398366\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"731.142236\" xlink:href=\"#ma82c802c45\" y=\"297.07783\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"736.463279\" xlink:href=\"#ma82c802c45\" y=\"296.64942\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"741.784321\" xlink:href=\"#ma82c802c45\" y=\"301.895287\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"747.105364\" xlink:href=\"#ma82c802c45\" y=\"298.065146\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"752.426407\" xlink:href=\"#ma82c802c45\" y=\"302.748631\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"757.747449\" xlink:href=\"#ma82c802c45\" y=\"300.485531\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"763.068492\" xlink:href=\"#ma82c802c45\" y=\"299.649171\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"768.389534\" xlink:href=\"#ma82c802c45\" y=\"298.974093\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"773.710577\" xlink:href=\"#ma82c802c45\" y=\"301.180939\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"779.03162\" xlink:href=\"#ma82c802c45\" y=\"300.418488\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"784.352662\" xlink:href=\"#ma82c802c45\" y=\"300.498206\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"789.673705\" xlink:href=\"#ma82c802c45\" y=\"301.860764\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"794.994747\" xlink:href=\"#ma82c802c45\" y=\"298.364018\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"800.31579\" xlink:href=\"#ma82c802c45\" y=\"300.716272\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"805.636832\" xlink:href=\"#ma82c802c45\" y=\"302.748196\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"810.957875\" xlink:href=\"#ma82c802c45\" y=\"305.34953\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"816.278918\" xlink:href=\"#ma82c802c45\" y=\"302.267191\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"821.59996\" xlink:href=\"#ma82c802c45\" y=\"301.675758\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"826.921003\" xlink:href=\"#ma82c802c45\" y=\"301.833883\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"832.242045\" xlink:href=\"#ma82c802c45\" y=\"301.433332\"/>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 33.2875 318.827216 \nL 33.2875 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 870.2875 318.827216 \nL 870.2875 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 33.2875 318.827216 \nL 870.2875 318.827216 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 33.2875 22.318125 \nL 870.2875 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_16\">\n    <!-- Training loss -->\n    <g transform=\"translate(414.620313 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M -0.296875 72.90625 \nL 61.375 72.90625 \nL 61.375 64.59375 \nL 35.5 64.59375 \nL 35.5 0 \nL 25.59375 0 \nL 25.59375 64.59375 \nL -0.296875 64.59375 \nz\n\" id=\"DejaVuSans-84\"/>\n      <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-84\"/>\n     <use x=\"46.333984\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"87.447266\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"148.726562\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"176.509766\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"239.888672\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"267.671875\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"331.050781\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"394.527344\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"426.314453\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"454.097656\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"515.279297\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"567.378906\" xlink:href=\"#DejaVuSans-115\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 33.2875 674.638125 \nL 870.2875 674.638125 \nL 870.2875 378.129034 \nL 33.2875 378.129034 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"xtick_9\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"71.332955\" xlink:href=\"#mb103619382\" y=\"674.638125\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 0 -->\n      <g transform=\"translate(68.151705 689.236562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"225.051963\" xlink:href=\"#mb103619382\" y=\"674.638125\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 20 -->\n      <g transform=\"translate(218.689463 689.236562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"378.770971\" xlink:href=\"#mb103619382\" y=\"674.638125\"/>\n      </g>\n     </g>\n     <g id=\"text_19\">\n      <!-- 40 -->\n      <g transform=\"translate(372.408471 689.236562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"532.489979\" xlink:href=\"#mb103619382\" y=\"674.638125\"/>\n      </g>\n     </g>\n     <g id=\"text_20\">\n      <!-- 60 -->\n      <g transform=\"translate(526.127479 689.236562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"686.208988\" xlink:href=\"#mb103619382\" y=\"674.638125\"/>\n      </g>\n     </g>\n     <g id=\"text_21\">\n      <!-- 80 -->\n      <g transform=\"translate(679.846488 689.236562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_14\">\n     <g id=\"line2d_21\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"839.927996\" xlink:href=\"#mb103619382\" y=\"674.638125\"/>\n      </g>\n     </g>\n     <g id=\"text_22\">\n      <!-- 100 -->\n      <g transform=\"translate(830.384246 689.236562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_23\">\n     <!-- Epoch -->\n     <g transform=\"translate(436.476562 702.914687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.8125 72.90625 \nL 55.90625 72.90625 \nL 55.90625 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.015625 \nL 54.390625 43.015625 \nL 54.390625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 8.296875 \nL 56.78125 8.296875 \nL 56.78125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-69\"/>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"63.183594\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"126.660156\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"187.841797\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"242.822266\" xlink:href=\"#DejaVuSans-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_7\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m7d6dba1f1f\" y=\"673.757917\"/>\n      </g>\n     </g>\n     <g id=\"text_24\">\n      <!-- 0.0 -->\n      <g transform=\"translate(10.384375 677.557135)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m7d6dba1f1f\" y=\"638.727239\"/>\n      </g>\n     </g>\n     <g id=\"text_25\">\n      <!-- 0.1 -->\n      <g transform=\"translate(10.384375 642.526457)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m7d6dba1f1f\" y=\"603.696561\"/>\n      </g>\n     </g>\n     <g id=\"text_26\">\n      <!-- 0.2 -->\n      <g transform=\"translate(10.384375 607.495779)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m7d6dba1f1f\" y=\"568.665883\"/>\n      </g>\n     </g>\n     <g id=\"text_27\">\n      <!-- 0.3 -->\n      <g transform=\"translate(10.384375 572.465101)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m7d6dba1f1f\" y=\"533.635204\"/>\n      </g>\n     </g>\n     <g id=\"text_28\">\n      <!-- 0.4 -->\n      <g transform=\"translate(10.384375 537.434423)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m7d6dba1f1f\" y=\"498.604526\"/>\n      </g>\n     </g>\n     <g id=\"text_29\">\n      <!-- 0.5 -->\n      <g transform=\"translate(10.384375 502.403745)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m7d6dba1f1f\" y=\"463.573848\"/>\n      </g>\n     </g>\n     <g id=\"text_30\">\n      <!-- 0.6 -->\n      <g transform=\"translate(10.384375 467.373067)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_14\">\n     <g id=\"line2d_29\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m7d6dba1f1f\" y=\"428.54317\"/>\n      </g>\n     </g>\n     <g id=\"text_31\">\n      <!-- 0.7 -->\n      <g transform=\"translate(10.384375 432.342389)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_15\">\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m7d6dba1f1f\" y=\"393.512492\"/>\n      </g>\n     </g>\n     <g id=\"text_32\">\n      <!-- 0.8 -->\n      <g transform=\"translate(10.384375 397.311711)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_31\">\n    <path clip-path=\"url(#pa3a9134de4)\" d=\"M 71.332955 661.160439 \nL 79.018905 659.383872 \nL 86.704855 653.731157 \nL 94.390806 650.016517 \nL 102.076756 647.432419 \nL 109.762707 640.487655 \nL 117.448657 637.257533 \nL 125.134607 633.865904 \nL 132.820558 629.020721 \nL 140.506508 622.721982 \nL 148.192459 613.839145 \nL 155.878409 604.956308 \nL 163.56436 597.204014 \nL 171.25031 588.482684 \nL 178.93626 577.015749 \nL 186.622211 569.101949 \nL 194.308161 559.573088 \nL 201.994112 551.820794 \nL 209.680062 544.714524 \nL 217.366012 537.446749 \nL 225.051963 531.14801 \nL 232.737913 522.103667 \nL 240.423864 516.127941 \nL 248.109814 509.50619 \nL 255.795764 502.39992 \nL 263.481715 495.778169 \nL 271.167665 489.640937 \nL 278.853616 482.534667 \nL 286.539566 478.335508 \nL 294.225517 474.943879 \nL 301.911467 471.552251 \nL 309.597417 467.676104 \nL 317.283368 463.315439 \nL 324.969318 461.054353 \nL 332.655269 456.855194 \nL 340.341219 454.594108 \nL 348.027169 453.463565 \nL 355.71312 452.494528 \nL 363.39907 451.040973 \nL 371.085021 448.133863 \nL 378.770971 445.872777 \nL 386.456921 444.257716 \nL 394.142872 441.99663 \nL 401.828822 440.543075 \nL 409.514773 439.574039 \nL 417.200723 437.312953 \nL 424.886674 435.859398 \nL 432.572624 434.08283 \nL 440.258574 433.598312 \nL 447.944525 432.144757 \nL 455.630475 430.691202 \nL 463.316426 429.399153 \nL 471.002376 427.784092 \nL 478.688326 425.846018 \nL 486.374277 425.3615 \nL 494.060227 424.230957 \nL 501.746178 422.777402 \nL 509.432128 421.485353 \nL 517.118079 419.870292 \nL 524.804029 419.224267 \nL 532.489979 419.062761 \nL 540.17593 416.478663 \nL 547.86188 415.994145 \nL 555.547831 414.379083 \nL 563.233781 414.379083 \nL 570.919731 413.894565 \nL 578.605682 412.44101 \nL 586.291632 412.764022 \nL 593.977583 412.117998 \nL 601.663533 411.148961 \nL 609.349483 408.887875 \nL 617.035434 408.403357 \nL 624.721384 407.918838 \nL 632.407335 406.303777 \nL 640.093285 406.142271 \nL 647.779236 403.881185 \nL 655.465186 404.204198 \nL 663.151136 403.235161 \nL 670.837087 402.589136 \nL 678.523037 401.943112 \nL 686.208988 400.812569 \nL 693.894938 400.651063 \nL 701.580888 399.843532 \nL 709.266839 399.359014 \nL 716.952789 398.874496 \nL 724.63874 398.712989 \nL 732.32469 398.228471 \nL 740.01064 396.774916 \nL 747.696591 397.259434 \nL 755.382541 396.290398 \nL 763.068492 395.967385 \nL 770.754442 395.482867 \nL 778.440393 394.836842 \nL 786.126343 394.190818 \nL 793.812293 393.867806 \nL 801.498244 392.898769 \nL 809.184194 392.737263 \nL 816.870145 393.060275 \nL 824.556095 391.60672 \nL 832.242045 391.929732 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <g clip-path=\"url(#pa3a9134de4)\">\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"71.332955\" xlink:href=\"#ma82c802c45\" y=\"661.160439\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"79.018905\" xlink:href=\"#ma82c802c45\" y=\"659.383872\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"86.704855\" xlink:href=\"#ma82c802c45\" y=\"653.731157\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"94.390806\" xlink:href=\"#ma82c802c45\" y=\"650.016517\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"102.076756\" xlink:href=\"#ma82c802c45\" y=\"647.432419\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"109.762707\" xlink:href=\"#ma82c802c45\" y=\"640.487655\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"117.448657\" xlink:href=\"#ma82c802c45\" y=\"637.257533\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"125.134607\" xlink:href=\"#ma82c802c45\" y=\"633.865904\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"132.820558\" xlink:href=\"#ma82c802c45\" y=\"629.020721\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"140.506508\" xlink:href=\"#ma82c802c45\" y=\"622.721982\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"148.192459\" xlink:href=\"#ma82c802c45\" y=\"613.839145\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"155.878409\" xlink:href=\"#ma82c802c45\" y=\"604.956308\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"163.56436\" xlink:href=\"#ma82c802c45\" y=\"597.204014\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"171.25031\" xlink:href=\"#ma82c802c45\" y=\"588.482684\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"178.93626\" xlink:href=\"#ma82c802c45\" y=\"577.015749\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"186.622211\" xlink:href=\"#ma82c802c45\" y=\"569.101949\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"194.308161\" xlink:href=\"#ma82c802c45\" y=\"559.573088\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"201.994112\" xlink:href=\"#ma82c802c45\" y=\"551.820794\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"209.680062\" xlink:href=\"#ma82c802c45\" y=\"544.714524\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"217.366012\" xlink:href=\"#ma82c802c45\" y=\"537.446749\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"225.051963\" xlink:href=\"#ma82c802c45\" y=\"531.14801\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"232.737913\" xlink:href=\"#ma82c802c45\" y=\"522.103667\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"240.423864\" xlink:href=\"#ma82c802c45\" y=\"516.127941\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"248.109814\" xlink:href=\"#ma82c802c45\" y=\"509.50619\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"255.795764\" xlink:href=\"#ma82c802c45\" y=\"502.39992\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"263.481715\" xlink:href=\"#ma82c802c45\" y=\"495.778169\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"271.167665\" xlink:href=\"#ma82c802c45\" y=\"489.640937\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"278.853616\" xlink:href=\"#ma82c802c45\" y=\"482.534667\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"286.539566\" xlink:href=\"#ma82c802c45\" y=\"478.335508\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"294.225517\" xlink:href=\"#ma82c802c45\" y=\"474.943879\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"301.911467\" xlink:href=\"#ma82c802c45\" y=\"471.552251\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"309.597417\" xlink:href=\"#ma82c802c45\" y=\"467.676104\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"317.283368\" xlink:href=\"#ma82c802c45\" y=\"463.315439\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"324.969318\" xlink:href=\"#ma82c802c45\" y=\"461.054353\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"332.655269\" xlink:href=\"#ma82c802c45\" y=\"456.855194\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"340.341219\" xlink:href=\"#ma82c802c45\" y=\"454.594108\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"348.027169\" xlink:href=\"#ma82c802c45\" y=\"453.463565\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"355.71312\" xlink:href=\"#ma82c802c45\" y=\"452.494528\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"363.39907\" xlink:href=\"#ma82c802c45\" y=\"451.040973\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"371.085021\" xlink:href=\"#ma82c802c45\" y=\"448.133863\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"378.770971\" xlink:href=\"#ma82c802c45\" y=\"445.872777\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"386.456921\" xlink:href=\"#ma82c802c45\" y=\"444.257716\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"394.142872\" xlink:href=\"#ma82c802c45\" y=\"441.99663\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"401.828822\" xlink:href=\"#ma82c802c45\" y=\"440.543075\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"409.514773\" xlink:href=\"#ma82c802c45\" y=\"439.574039\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"417.200723\" xlink:href=\"#ma82c802c45\" y=\"437.312953\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"424.886674\" xlink:href=\"#ma82c802c45\" y=\"435.859398\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"432.572624\" xlink:href=\"#ma82c802c45\" y=\"434.08283\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"440.258574\" xlink:href=\"#ma82c802c45\" y=\"433.598312\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"447.944525\" xlink:href=\"#ma82c802c45\" y=\"432.144757\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"455.630475\" xlink:href=\"#ma82c802c45\" y=\"430.691202\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"463.316426\" xlink:href=\"#ma82c802c45\" y=\"429.399153\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"471.002376\" xlink:href=\"#ma82c802c45\" y=\"427.784092\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"478.688326\" xlink:href=\"#ma82c802c45\" y=\"425.846018\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"486.374277\" xlink:href=\"#ma82c802c45\" y=\"425.3615\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"494.060227\" xlink:href=\"#ma82c802c45\" y=\"424.230957\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"501.746178\" xlink:href=\"#ma82c802c45\" y=\"422.777402\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"509.432128\" xlink:href=\"#ma82c802c45\" y=\"421.485353\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"517.118079\" xlink:href=\"#ma82c802c45\" y=\"419.870292\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"524.804029\" xlink:href=\"#ma82c802c45\" y=\"419.224267\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"532.489979\" xlink:href=\"#ma82c802c45\" y=\"419.062761\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"540.17593\" xlink:href=\"#ma82c802c45\" y=\"416.478663\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"547.86188\" xlink:href=\"#ma82c802c45\" y=\"415.994145\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"555.547831\" xlink:href=\"#ma82c802c45\" y=\"414.379083\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"563.233781\" xlink:href=\"#ma82c802c45\" y=\"414.379083\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"570.919731\" xlink:href=\"#ma82c802c45\" y=\"413.894565\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"578.605682\" xlink:href=\"#ma82c802c45\" y=\"412.44101\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"586.291632\" xlink:href=\"#ma82c802c45\" y=\"412.764022\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"593.977583\" xlink:href=\"#ma82c802c45\" y=\"412.117998\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"601.663533\" xlink:href=\"#ma82c802c45\" y=\"411.148961\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"609.349483\" xlink:href=\"#ma82c802c45\" y=\"408.887875\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"617.035434\" xlink:href=\"#ma82c802c45\" y=\"408.403357\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"624.721384\" xlink:href=\"#ma82c802c45\" y=\"407.918838\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"632.407335\" xlink:href=\"#ma82c802c45\" y=\"406.303777\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"640.093285\" xlink:href=\"#ma82c802c45\" y=\"406.142271\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"647.779236\" xlink:href=\"#ma82c802c45\" y=\"403.881185\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"655.465186\" xlink:href=\"#ma82c802c45\" y=\"404.204198\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"663.151136\" xlink:href=\"#ma82c802c45\" y=\"403.235161\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"670.837087\" xlink:href=\"#ma82c802c45\" y=\"402.589136\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"678.523037\" xlink:href=\"#ma82c802c45\" y=\"401.943112\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"686.208988\" xlink:href=\"#ma82c802c45\" y=\"400.812569\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"693.894938\" xlink:href=\"#ma82c802c45\" y=\"400.651063\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"701.580888\" xlink:href=\"#ma82c802c45\" y=\"399.843532\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"709.266839\" xlink:href=\"#ma82c802c45\" y=\"399.359014\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"716.952789\" xlink:href=\"#ma82c802c45\" y=\"398.874496\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"724.63874\" xlink:href=\"#ma82c802c45\" y=\"398.712989\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"732.32469\" xlink:href=\"#ma82c802c45\" y=\"398.228471\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"740.01064\" xlink:href=\"#ma82c802c45\" y=\"396.774916\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"747.696591\" xlink:href=\"#ma82c802c45\" y=\"397.259434\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"755.382541\" xlink:href=\"#ma82c802c45\" y=\"396.290398\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"763.068492\" xlink:href=\"#ma82c802c45\" y=\"395.967385\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"770.754442\" xlink:href=\"#ma82c802c45\" y=\"395.482867\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"778.440393\" xlink:href=\"#ma82c802c45\" y=\"394.836842\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"786.126343\" xlink:href=\"#ma82c802c45\" y=\"394.190818\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"793.812293\" xlink:href=\"#ma82c802c45\" y=\"393.867806\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"801.498244\" xlink:href=\"#ma82c802c45\" y=\"392.898769\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"809.184194\" xlink:href=\"#ma82c802c45\" y=\"392.737263\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"816.870145\" xlink:href=\"#ma82c802c45\" y=\"393.060275\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"824.556095\" xlink:href=\"#ma82c802c45\" y=\"391.60672\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"832.242045\" xlink:href=\"#ma82c802c45\" y=\"391.929732\"/>\n    </g>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 33.2875 674.638125 \nL 33.2875 378.129034 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 870.2875 674.638125 \nL 870.2875 378.129034 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 33.2875 674.638125 \nL 870.2875 674.638125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 33.2875 378.129034 \nL 870.2875 378.129034 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_33\">\n    <!-- Accuracy -->\n    <g transform=\"translate(424.39375 372.129034)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 34.1875 63.1875 \nL 20.796875 26.90625 \nL 47.609375 26.90625 \nz\nM 28.609375 72.90625 \nL 39.796875 72.90625 \nL 67.578125 0 \nL 57.328125 0 \nL 50.6875 18.703125 \nL 17.828125 18.703125 \nL 11.1875 0 \nL 0.78125 0 \nz\n\" id=\"DejaVuSans-65\"/>\n      <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n      <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-65\"/>\n     <use x=\"66.658203\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"121.638672\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"176.619141\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"239.998047\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"281.111328\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"342.390625\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"397.371094\" xlink:href=\"#DejaVuSans-121\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_12\">\n     <path d=\"M 791.835938 669.638125 \nL 863.2875 669.638125 \nQ 865.2875 669.638125 865.2875 667.638125 \nL 865.2875 653.96 \nQ 865.2875 651.96 863.2875 651.96 \nL 791.835938 651.96 \nQ 789.835938 651.96 789.835938 653.96 \nL 789.835938 667.638125 \nQ 789.835938 669.638125 791.835938 669.638125 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_32\">\n     <path d=\"M 793.835938 660.058437 \nL 813.835938 660.058437 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_33\">\n     <g>\n      <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"803.835938\" xlink:href=\"#ma82c802c45\" y=\"660.058437\"/>\n     </g>\n    </g>\n    <g id=\"text_34\">\n     <!-- Training -->\n     <g transform=\"translate(821.835938 663.558437)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-84\"/>\n      <use x=\"46.333984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"87.447266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"148.726562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"176.509766\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"239.888672\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"267.671875\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"331.050781\" xlink:href=\"#DejaVuSans-103\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pb954fbb882\">\n   <rect height=\"296.509091\" width=\"837\" x=\"33.2875\" y=\"22.318125\"/>\n  </clipPath>\n  <clipPath id=\"pa3a9134de4\">\n   <rect height=\"296.509091\" width=\"837\" x=\"33.2875\" y=\"378.129034\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "opt_params, loss_hist, train_acc_hist = results\n",
    "\n",
    "# Plot the learning curves\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss')\n",
    "loss_hist_ = loss_hist[1::100]  # sparse the curve a bit\n",
    "plt.plot(loss_hist_, '-o')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(train_acc_hist, '-o', label='Training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can generate text using the trained model. You can start from a specific word in the original text, such as `she`. (We expect the text to not be too repetitive, i.e. not repeating the same three words over and over. See an example of an acceptable sample below.)\n",
    "\n",
    "> she was dozing off, and book-shelves; here and she tried to curtsey as she spoke--fancy curtseying as youre falling through the little door into a dreamy sort of way, do cats eat bats? do cats eat bats? and sometimes,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "she was too slippery; and when she was coming to, but it was nothing else to be seen: she had tired herself out with trying, the rabbit actually took to herself, after such a little glass box that was nothing\n"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "# you can change the generated text length below.\n",
    "text_length = 40\n",
    "\n",
    "idx = 0\n",
    "# you also can start from specific word. \n",
    "# since the words are all converted into lower case\n",
    "idx = int(np.where(np.asarray(word_list) == 'She'.lower())[0])\n",
    "\n",
    "# sample from the trained model\n",
    "words = model.sample(idx, text_length-1)\n",
    "\n",
    "# convert indices into words\n",
    "output = [word_list[i] for i in words]\n",
    "print(' '.join(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inline Question (2 Pts): Play around with different settings to get better understanding of its behavior and describe your observation. Make sure to cover at least the following points:\n",
    "* Vanilla RNN vs LSTM (you can set different training timesteps `T` and `test_length` to test with longer texts.)\n",
    "* Limitations you observed when training the recurrent language models. What could be causing them? (there's no unique answer. just explain your own opinion from experiments.)\n",
    "\n",
    "(Please limit your answer to <150 words)\n",
    "\n",
    "#### Ans: \n",
    "\n",
    "While playing around with different parameters, I observed two interesting patterns. First one being, the LSTM performs better with larger timesteps but is not so good with smaller timesteps. However, for RNN, the behavior is reverse i.e. RNN performs worse with the larger timesteps and does a good job with smaller ones. I think this is primarily because of the vanishing gradient issue which happens with deeper networks in RNN.  \n",
    "\n",
    "The second thing I observed in both the models (but more in LSTM) is repetition of text when I increase the test_length. I suspect this is happening becuase of lack of randomization. We may be able mitigate this issue by including randomization through dropout layers. Also, instead of using top choice as our predicted word which results in extremely repetitive and predictable text, we can treat the output of the model as a probability distribution and sample from that to simulate more to the real world behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "Please prepare a PDF document `problem_2_solution.pdf` in the root directory of this repository with all plots and inline answers of your solution. Concretely, the document should contain the following items in strict order:\n",
    "1. Training loss / accuracy curves for vanilla RNN and LSTM training\n",
    "2. Sample text generation from a trained model\n",
    "3. Answers to inline questions about recurrent net behavior\n",
    "\n",
    "Note that you still need to submit the jupyter notebook with all generated solutions. We will randomly pick submissions and check that the plots in the PDF and in the notebook are equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Text Corpora (Not Graded)\n",
    "If you want to explore the capabilities of your model further, feel free to train the model on new text corpora! Just bring them in the appropriate format (see above) and then train your model. You can change any of the hyperparameters in the code blocks below. If your model produces some fun outputs you can print them below (including the text corpus the model was trained on).\n",
    "\n",
    "**! Note that this is completely optional and has no influence on the grade of the assignment. !**\n",
    "\n",
    "**! Please make sure that all notebook blocks above show results trained on the original text dataset for the assignment as we can only grade those. !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "################ LOAD YOUR DATA HERE ####################\n",
    "# word_list: list of unique vocabulary entries, e.g. ['', 'country', 'slippery;', 'hurt,', 'long', ...]\n",
    "# data: integer array of shape (num_data_samples,), \n",
    "# holds index into word_list for each word in the input text, e.g. [649 377 263 ... 179 148 0]\n",
    "word_list, data = None, None\n",
    "#########################################################\n",
    "\n",
    "gt_labels = data[1:]\n",
    "input_data = data[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "# you can change the following parameters.\n",
    "D = 10  # input dimention\n",
    "H = 20  # hidden space dimention\n",
    "T = 50  # timesteps\n",
    "N = 10  # batch size\n",
    "max_epoch = 50  # max epoch size\n",
    "\n",
    "loss_func = temporal_softmax_CE_loss()\n",
    "# you can change the cell_type between 'rnn' and 'lstm'.\n",
    "model = LanguageModelRNN(dataSize, D, H, cell_type='lstm')\n",
    "optimizer = Adam(model, 5e-4)\n",
    "\n",
    "data = {'data_train': input_data, 'labels_train': gt_labels}\n",
    "\n",
    "results = train_net(data, model, loss_func, optimizer, timesteps=T, batch_size=N, max_epochs=max_epoch, verbose=True)\n",
    "\n",
    "opt_params, loss_hist, train_acc_hist = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "# Plot the learning curves\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss')\n",
    "loss_hist_ = loss_hist[1::100]  # sparse the curve a bit\n",
    "plt.plot(loss_hist_, '-o')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(train_acc_hist, '-o', label='Training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "# you can change the generated text length below.\n",
    "text_length = 15\n",
    "\n",
    "idx = 0\n",
    "# you also can start from specific word. \n",
    "# since the words are all converted into lower case\n",
    "idx = int(np.where(np.asarray(word_list) == 'She'.lower())[0])\n",
    "\n",
    "# sample from the trained model\n",
    "words = model.sample(idx, text_length-1)\n",
    "\n",
    "# convert indices into words\n",
    "output = [word_list[i] for i in words]\n",
    "print(' '.join(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fun text generations?\n",
    "[...]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.8.3 64-bit ('dl_env': venv)",
   "display_name": "Python 3.8.3 64-bit ('dl_env': venv)",
   "metadata": {
    "interpreter": {
     "hash": "3687cfb02597ad25d60753771e355ee763c21e19868c48b521ac500449d295d6"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}